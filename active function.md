### 1.为什么要用激活函数？

如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机（Perceptron）。没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。

如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。

### 2.激活函数为什么是非线性的？

如果使用线性激活函数，那么输入跟输出之间的关系为线性的，无论神经网络有多少层都是线性组合。

**使用非线性激活函数是为了增加神经网络模型的非线性因素**，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。

输出层可能会使用线性激活函数，但在隐含层都使用非线性激活函数。
 
### 3.常用激活函数：sigmoid，Tanh，ReLU，Leaky ReLU，PReLU，ELU，Maxout

#### （1）sigmiod函数  
sigmoid函数又称 Logistic函数，用于隐层神经元输出，取值范围为(0,1)，可以用来做二分类。

sigmoid函数表达式：  
![image](https://www.zhihu.com/equation?tex=%5Csigma%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D)   
它的导数为：  
![image](https://pic1.zhimg.com/80/v2-e2fbdc1a61f770bc462d0622b65fe66c_1440w.jpg)  

sigmoid函数的几何形状是一条S型曲线，图像如下：  
![image](https://img-blog.csdnimg.cn/img_convert/e1f0517ce6d38715fc5eeb14ad014eea.png)  

* 优点：

* 平滑且可微：Sigmoid函数的平滑S形曲线在神经网络中具有很好的性质，因为它可以帮助优化器找到全局最小值。另外，Sigmoid函数在任何地方都是可微的，这使得神经网络的梯度可以通过反向传播来计算和更新。

* 输出值映射到0到1之间：Sigmoid函数的输出被映射到0到1的范围内，因此可以被解释为概率分布。因此，它通常用于**二元分类问题的输出层**。

* 基于概率的输出：由于Sigmoid函数的输出范围被限制在0到1之间，因此可以将其解释为概率分布，这对于一些任务（如二元分类）非常有用。

* 缺点：

* 容易出现梯度消失问题：Sigmoid函数的**导数在输入接近饱和区域时非常小**，这会导致在深层神经网络中出现**梯度消失的问题**。这是因为在反向传播时，由于导数非常小，梯度会迅速消失，并且在达到较低层时可能已经几乎为零，因此这些层将无法更新权重。

* 输出不以0为中心：Sigmoid函数的输出不是以0为中心的，这会导致在训练过程中**难以使神经网络收敛**。如果输入的均值不为0，则Sigmoid函数的输出可能会偏向其中一个极端。

* 计算代价高：计算Sigmoid函数的代价相对较高，因为它包含指数运算，而指数运算的计算代价相对较高。

* **综上所述**，尽管Sigmoid函数有一些优点，但由于其存在的缺陷，它在深度神经网络中的使用已经有所减少。在大多数情况下，ReLU或其变体（如Leaky ReLU）更常用，因为它们可以有效地解决Sigmoid函数的问题，并具有更好的性能表现。

#### （2）Tanh函数  
Tanh函数也称为双曲正切函数，取值范围为[-1,1]  
Tanh函数定义如下：  

![image](https://img-blog.csdnimg.cn/20210302142654537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MDk1OTg5MA==,size_16,color_FFFFFF,t_70)  
它的导数为：  
![image](https://pic1.zhimg.com/80/v2-a1850908aacbfdcabcc551ecc9196b14_1440w.jpg)  
他的图像为：  
![image](https://img-blog.csdnimg.cn/img_convert/aff10bebd3a3a599c044499eed34f70a.png)  

* 优点：

* 输出值映射到-1到1之间：Tanh函数的输出范围为-1到1之间，因此它可以在神经网络中用作任何范围的激活函数。

* 输出以0为中心：**Tanh函数的输出以0为中心，这可以减轻神经网络训练过程中的偏置问题**，使得神经网络更容易学习输入数据的特征。

* 平滑且可微：Tanh函数与Sigmoid函数相似，其平滑的S形曲线在神经网络中具有很好的性质，可以帮助优化器找到全局最小值。并且，Tanh函数在任何地方都是可微的，这使得神经网络的梯度可以通过反向传播来计算和更新。

* 缺点：

* 容易出现梯度消失问题：与Sigmoid函数类似，Tanh函数的导数在输入接近饱和区域时非常小，这会导致在深层神经网络中出现梯度消失的问题。

* 计算代价高：计算Tanh函数的代价相对较高，因为它包含指数运算，而指数运算的计算代价相对较高。

* 综上所述，尽管Tanh函数在一些情况下可能比Sigmoid函数更优越，但它仍然存在与Sigmoid函数相同的缺陷，因此在深度神经网络中，通常使用ReLU或其变体（如Leaky ReLU）作为激活函数，因为它们可以解决梯度消失问题，并具有更好的性能表现。
