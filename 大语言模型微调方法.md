* **Fine-Tuning**  微调，一般指全参数的微调 (全量微调) ，指是一类较早诞生的微调方Fine-Tuning，法，全参数微调需要消耗大量的算力，实际使用起来并不方便，因此不久之后又诞生了只围绕部分参数进行微调的高效微调方法  
* 高效微调，State-of-the-art Parameter-Efficient Fine-Tuning (**SOTA PEFT**),特指部分参数的微调方法，这种方法算力功耗比更高，也是目前最为常见的微调方法  
* 除此之外，Fine-Tuning也可以代指全部微调方法，同时OpenAl中模型微调API的名称也是Fine-Tuning,需要注意的是，OpenAI提供的在线微调方法也是一种高效微调方法，并不是全量微调;
* **基于强化学习的进阶微调方法RLHF方法**  [论文地址](https://arxiv.org/abs/2203.02155)  RLHF: Reinforcement Learning from HumanFeedback，即基于人工反馈机制的强化学习。最早与2022年4月，由OpenAI研究团队系统总结并提出，并在GPT模型的对话类任务微调中大放异彩，被称为ChatGPT“背后的功臣”;
RLHF也是目前为止常用的、最为复杂的基于强化学习的大语言模型微调方法，目前最好的端到端RIHF由微软开源并维护
* **LORA**: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS (2021)基于低阶自适应的大语言模型微调方法
* LORA除了可以用于微调大语言模型 (LLM) 外，目前还有一个非常火爆的应用场景: 围绕diffusion models (扩散模型) 进行微调，并在图片生成任务中表现惊艳;
* **高效微调方法二: Prefix Tuning**  [论文地址](https://aclanthology.org/2021.acl-long.353/0)
Prefix-Tuning: Optimizing Continuous Prompts for Generation (2021)
基于提示词前缀优化的微调方法
来源于斯坦福大学的一种高效微调方法
原理简述: 在原始模型基础上，增加一个可被训练的Embedding层，用于给提示词增加前缀，从而让模型更好的理解提示词意图并在训练过程中不断优化这些参数;
Prefix Tuning既能够在模型结构上增加一些新的灵活性，又能够能够改进模型表现的提天机制
* **高效微调方法三: Prompt Tuning**
[论文地址]( https://arxiv.org/abs/2104.08691)
The Power of Scale for Parameter-Efficient Prompt Tuning (2021)
由谷歌提出的一种轻量级的优化方法;
原理简述:该方法相当于是Prefix Tuning的简化版本，即无需调整模型参数，而是在已有的参数中，选择一部分参数作为可学习参数，用于创建每个Prompt的前缀，从而帮助模型更好地理解和处理特定的任务;
不同于Prefix方法，Prompt Tuning训练得到的前缀是具备可解释性的我们可以通过查看这些前缀，来查看模型是如何帮我们优化prompte该方法在参数规模非常大的模型微调时效果很好，当参数规模达到九天Hectorl100亿时和全量微调效果一致;
* **高效微调方法四: P-Tuning v2**
[GitHub地址](https://github.com/THUDM/P-tuning-v20)   [论文地址](https://aclanthology.org/2021.acl-long.353/ChatGLM-6B+P-Tuning)   [微调项目地址](https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md)  
P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuningUniversally Across Scales and Tasks (2022)
  来源于清华大学团队提出的高效微调方法;  
  原理简述: 可以理解为Prefix tuning的改进版本，即P-Tuning v2不仅在输入层添加了连续的prompts (可被训练的Embedding层)，而且还在预训练模型的每一层都添加了连续的prompts;
  这种深度的prompt tuning增加了连续prompts的容量，出于某些原因，P-Tuning v2会非常适合GLM这种双向预训练大模型微调
